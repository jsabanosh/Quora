{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jsaba\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\jsaba\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\jsaba\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:855: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "import time\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn import cross_validation, metrics\n",
    "from nltk.corpus import stopwords\n",
    "import timeit\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "import xgboost as xgb\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import sparse\n",
    "from nltk import ngrams, word_tokenize\n",
    "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor\n",
    "from nltk import stem, pos_tag, pos_tag_sents\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import gensim\n",
    "from pandas import HDFStore\n",
    "%matplotlib inline\n",
    "%load_ext cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "tagger = PerceptronTagger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Run Attributes\n",
    "maxGram=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "just_train = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/train.csv')\n",
    "if not just_train:\n",
    "    test = pd.read_csv('../input/test.csv')\n",
    "    test=test.fillna('')\n",
    "    test['isTrain']=test.apply(lambda row: 0, axis=1)\n",
    "sample = pd.read_csv('../input/sample_submission.csv')\n",
    "train=train.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['isTrain']=train.apply(lambda row: 1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stops=stopwords.words(\"english\")+['']\n",
    "strPunc=string.punctuation + '“”'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"'\" in train.loc[24,'question2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "countries = pd.read_csv('../input/countries.csv',sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coList = countries['Country (en)'].tolist()\n",
    "coKeys = [x.lower() for x in coList]\n",
    "coValues = countries['Country code'].fillna('').tolist()\n",
    "coDict = dict(zip(coKeys, coValues))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Do not Lower these due to country code caps\n",
    "\n",
    "def coCos(row):\n",
    "    q1=row['question1']\n",
    "    q2=row['question2']\n",
    "    matchCount=0\n",
    "    failCount=0\n",
    "    hasCountry=0\n",
    "    for co, code in coDict.items():\n",
    "        if co in q1.lower():\n",
    "            hasCountry=1\n",
    "            if co in q2.lower():\n",
    "                matchCount=matchCount+1\n",
    "            elif code in q2:\n",
    "                matchCount=matchCount+1\n",
    "            else:\n",
    "                failCount=failCount+1\n",
    "        elif co in q2.lower():\n",
    "            hasCountry=1\n",
    "            if code not in q1:\n",
    "                failCount=failCount+1\n",
    "            else:\n",
    "                matchCount=matchCount+1\n",
    "    return matchCount, failCount, hasCountry\n",
    "\n",
    "def coNums(row):\n",
    "    q1=row['q1Nums']\n",
    "    q2=row['q2Nums']\n",
    "    matchCount=0\n",
    "    failCount=0\n",
    "    hasNum=0\n",
    "    if len(q1) > 0:\n",
    "        hasNum=1\n",
    "        for num in q1:\n",
    "            if num in q2:\n",
    "                matchCount=matchCount+1\n",
    "            else:\n",
    "                failCount=failCount+1\n",
    "    if len(q2) > 0:\n",
    "        hasNum=1\n",
    "        for num in [x for x in q2 if x not in q1]:\n",
    "            failCount=failCount+1\n",
    "            \n",
    "    return matchCount, failCount, hasNum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# select=(train['matchCount']>0) | (train['failCount']>0)\n",
    "# train[['question1','question2','is_duplicate','matchCount','failCount']][select].to_csv('../output/train.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#del tempTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "abbr_dict={\n",
    "    \"what's\":\"what is\",\n",
    "    \"what're\":\"what are\",\n",
    "    \"who's\":\"who is\",\n",
    "    \"who're\":\"who are\",\n",
    "    \"where's\":\"where is\",\n",
    "    \"where're\":\"where are\",\n",
    "    \"when's\":\"when is\",\n",
    "    \"when're\":\"when are\",\n",
    "    \"how's\":\"how is\",\n",
    "    \"how're\":\"how are\",\n",
    "\n",
    "    \"i'm\":\"i am\",\n",
    "    \"we're\":\"we are\",\n",
    "    \"you're\":\"you are\",\n",
    "    \"they're\":\"they are\",\n",
    "    \"it's\":\"it is\",\n",
    "    \"he's\":\"he is\",\n",
    "    \"she's\":\"she is\",\n",
    "    \"that's\":\"that is\",\n",
    "    \"there's\":\"there is\",\n",
    "    \"there're\":\"there are\",\n",
    "\n",
    "    \"i've\":\"i have\",\n",
    "    \"we've\":\"we have\",\n",
    "    \"you've\":\"you have\",\n",
    "    \"they've\":\"they have\",\n",
    "    \"who've\":\"who have\",\n",
    "    \"would've\":\"would have\",\n",
    "    \"not've\":\"not have\",\n",
    "\n",
    "    \"i'll\":\"i will\",\n",
    "    \"we'll\":\"we will\",\n",
    "    \"you'll\":\"you will\",\n",
    "    \"he'll\":\"he will\",\n",
    "    \"she'll\":\"she will\",\n",
    "    \"it'll\":\"it will\",\n",
    "    \"they'll\":\"they will\",\n",
    "\n",
    "    \"isn't\":\"is not\",\n",
    "    \"wasn't\":\"was not\",\n",
    "    \"aren't\":\"are not\",\n",
    "    \"weren't\":\"were not\",\n",
    "    \"can't\":\"can not\",\n",
    "    \"couldn't\":\"could not\",\n",
    "    \"don't\":\"do not\",\n",
    "    \"didn't\":\"did not\",\n",
    "    \"shouldn't\":\"should not\",\n",
    "    \"wouldn't\":\"would not\",\n",
    "    \"doesn't\":\"does not\",\n",
    "    \"haven't\":\"have not\",\n",
    "    \"hasn't\":\"has not\",\n",
    "    \"hadn't\":\"had not\",\n",
    "    \"won't\":\"will not\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~“”'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strPunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "st = stem.snowball.PorterStemmer()\n",
    "lmtzr = WordNetLemmatizer()\n",
    "def psWA(word): #Included because words like \"AED\" fail with exception on porterstemmer\n",
    "    try:\n",
    "        return st.stem(word)\n",
    "    except:\n",
    "        return word\n",
    "    \n",
    "def puncRepl(c):\n",
    "    if c in \"'-_\":\n",
    "        return ''\n",
    "    elif c in strPunc:\n",
    "        return ' '\n",
    "    else:\n",
    "        return c\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN #Because noun is the wordnet default, possibly just skip these\n",
    "    \n",
    "def pos_trans(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return 'Adj'\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return 'Verb'\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return 'Noun'\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return 'Adv'\n",
    "    else:\n",
    "        return 'None'#Because noun is the wordnet default, possibly just skip these\n",
    "\n",
    "def multiwordReplace(text, wordDic):\n",
    "    \"\"\"\n",
    "    take a text and replace words that match a key in a dictionary with\n",
    "    the associated value, return the changed text\n",
    "    \"\"\"\n",
    "    rc = re.compile('|'.join(map(re.escape, wordDic)))\n",
    "    def translate(match):\n",
    "        return wordDic[match.group(0)]\n",
    "    return rc.sub(translate, text)\n",
    "    \n",
    "def cleanQ(Q):\n",
    "    Q = Q.lower()\n",
    "    Q=multiwordReplace(Q, abbr_dict)\n",
    "    Q = ''.join([puncRepl(x) for x in Q]) #change this to RE with punc followed by whitespace\n",
    "    \n",
    "    QC = [x for x in Q.split(' ') if x not in ''] #if x not in stops]\n",
    "    Q=QC.copy()\n",
    "    QC = ' '.join(QC)\n",
    "    \n",
    "    QT = tagger.tag(Q)\n",
    "    QA = [(psWA(x), lmtzr.lemmatize(x,pos=get_wordnet_pos(y)),y) for x,y in QT]\n",
    "    \n",
    "    QL = [w for w,x,y in QA]\n",
    "    QS = [x for w,x,y in QA]\n",
    "    QSE = ' '.join(QL)\n",
    "    QLE = ' '.join(QS)\n",
    "    \n",
    "    NounsL = ' '.join([x for w,x,y in QA if pos_trans(y) == 'Noun'])\n",
    "    VerbsL = ' '.join([x for w,x,y in QA if pos_trans(y) == 'Verb'])\n",
    "    AdjsL = ' '.join([x for w,x,y in QA if pos_trans(y) == 'Adj'])\n",
    "    AdvsL = ' '.join([x for w,x,y in QA if pos_trans(y) == 'Adv'])\n",
    "    \n",
    "    NounsS = ' '.join([w for w,x,y in QA if pos_trans(y) == 'Noun'])\n",
    "    VerbsS = ' '.join([w for w,x,y in QA if pos_trans(y) == 'Verb'])\n",
    "    AdjsS = ' '.join([w for w,x,y in QA if pos_trans(y) == 'Adj'])\n",
    "    AdvsS = ' '.join([w for w,x,y in QA if pos_trans(y) == 'Adv'])\n",
    "    \n",
    "    return QC, QSE, QLE, NounsL, NounsS, VerbsL, VerbsS, AdjsL, AdjsS, AdvsL, AdvsS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('this is a test of a fairly typical sentence and what is this one of a harder to parse sentence mitchell',\n",
       " 'thi is a test of a fairli typic sentenc and what is thi one of a harder to pars sentenc mitchel',\n",
       " 'this be a test of a fairly typical sentence and what be this one of a harder to parse sentence mitchell',\n",
       " 'test sentence one harder sentence mitchell',\n",
       " 'test sentenc one harder sentenc mitchel',\n",
       " 'be be parse',\n",
       " 'is is pars',\n",
       " 'typical',\n",
       " 'typic',\n",
       " 'fairly',\n",
       " 'fairli')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanQ(\"This is a test of a fairly typical sentence. And what's this?... one/of a harder/to parse sentence, Mitchell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "cleanQ(\"exceptionally rigidly hard to parse/understand sentences. And  'another' one\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if just_train:\n",
    "    TT = train\n",
    "else:\n",
    "    TT=train.append(test)\n",
    "TT=TT.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TT.question1 = TT.question1.astype(str)\n",
    "TT.question2 = TT.question2.astype(str)\n",
    "\n",
    "start = time.time()\n",
    "TT.question1.replace(abbr_dict,inplace=True)\n",
    "TT.question2.replace(abbr_dict,inplace=True)\n",
    "duration = time.time() - start\n",
    "print(str(int(duration//60)) + \" Minute(s), \" + str(int(duration%60)) + \" Second(s).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ends = ['C','S','L','NounsL', 'NounsS', 'VerbsL', 'VerbsS', 'AdjsL', 'AdjsS', 'AdvsL', 'AdvsS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#######Setup Train Qs#######\n",
    "start = time.time()\n",
    "#Clean and Stem Rows\n",
    "\n",
    "train_qs=pd.DataFrame()\n",
    "\n",
    "(TT['q1C'],TT['q1S'],TT['q1L'],TT['q1NounsL'],TT['q1NounsS'],TT['q1VerbsL'],TT['q1VerbsS'],\n",
    " TT['q1AdjsL'],TT['q1AdjsS'],TT['q1AdvsL'],TT['q1AdvsS']\n",
    ")=zip(*TT.apply(lambda row: cleanQ(row['question1']),axis=1))\n",
    "(TT['q2C'],TT['q2S'],TT['q2L'],TT['q2NounsL'],TT['q2NounsS'],TT['q2VerbsL'],TT['q2VerbsS'],\n",
    " TT['q2AdjsL'],TT['q2AdjsS'],TT['q2AdvsL'],TT['q2AdvsS']\n",
    ")=zip(*TT.apply(lambda row: cleanQ(row['question2']),axis=1))\n",
    "\n",
    "duration = time.time() - start\n",
    "print(str(int(duration//60)) + \" Minute(s), \" + str(int(duration%60)) + \" Second(s).\")\n",
    "\n",
    "train_qs['C'] = pd.Series(TT['q1C'].tolist() + TT['q2C'].tolist()).astype(str)\n",
    "train_qs['S'] = pd.Series(TT['q1S'].tolist() + TT['q2S'].tolist()).astype(str)\n",
    "train_qs['L'] = pd.Series(TT['q1L'].tolist() + TT['q2L'].tolist()).astype(str)\n",
    "train_qs['NounsL'] = pd.Series(TT['q1NounsL'].tolist() + TT['q2NounsL'].tolist()).astype(str)\n",
    "train_qs['VerbsL'] = pd.Series(TT['q1VerbsL'].tolist() + TT['q2VerbsL'].tolist()).astype(str)\n",
    "train_qs['AdjsL'] = pd.Series(TT['q1AdjsL'].tolist() + TT['q2AdjsL'].tolist()).astype(str)\n",
    "train_qs['AdvsL'] = pd.Series(TT['q1AdvsL'].tolist() + TT['q2AdvsL'].tolist()).astype(str)\n",
    "train_qs['NounsS'] = pd.Series(TT['q1NounsS'].tolist() + TT['q2NounsS'].tolist()).astype(str)\n",
    "train_qs['VerbsS'] = pd.Series(TT['q1VerbsS'].tolist() + TT['q2VerbsS'].tolist()).astype(str)\n",
    "train_qs['AdjsS'] = pd.Series(TT['q1AdjsS'].tolist() + TT['q2AdjsS'].tolist()).astype(str)\n",
    "train_qs['AdvsS'] = pd.Series(TT['q1AdvsS'].tolist() + TT['q2AdvsS'].tolist()).astype(str)\n",
    "\n",
    "duration = time.time() - start\n",
    "print(str(int(duration//60)) + \" Minute(s), \" + str(int(duration%60)) + \" Second(s).\")\n",
    "\n",
    "TT['q1Nums']=TT.apply(lambda row: ' '.join(re.findall(\"[0-9.]+\", re.sub(',','',row['question1']))),axis=1)\n",
    "TT['q2Nums']=TT.apply(lambda row: ' '.join(re.findall(\"[0-9.]+\", re.sub(',','',row['question2']))),axis=1)\n",
    "\n",
    "duration = time.time() - start\n",
    "print(str(int(duration//60)) + \" Minute(s), \" + str(int(duration%60)) + \" Second(s).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "re.findall(\"[0-9.]+\",re.sub(\"[0-9]k\",\"000\",'this is a number 5009.8 and it is not 60043 and 8k dollars in money'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "qt=\"hard to parse/understand sentence. And  'another' one\"\n",
    "cleanQ(qt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del train\n",
    "if not just_train:\n",
    "    del test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "for end in ends:\n",
    "    counter=0\n",
    "    nDocsDict = {}\n",
    "    for i in range(1,maxGram+1):\n",
    "        nDocsDict[i]={}\n",
    "    for row in train_qs[end]:\n",
    "        phraseList=[]\n",
    "        question = row\n",
    "        question=question.split()\n",
    "        for i in range(1,maxGram+1):\n",
    "            for count in range(0,len(question)-(i-1)):\n",
    "                phrase = ' '.join(question[count:count+i])\n",
    "                if phrase not in phraseList:\n",
    "                    if phrase in nDocsDict[i]:\n",
    "                        nDocsDict[i][phrase] = nDocsDict[i][phrase] + 1\n",
    "                    else:\n",
    "                        nDocsDict[i][phrase] = 1\n",
    "                    phraseList = phraseList+[phrase]\n",
    "    \n",
    "    nDocs=train_qs[end].shape[0]\n",
    "    \n",
    "    weights[end]={}\n",
    "    for i in range(1,maxGram+1):\n",
    "        weights[end][i] = dict((k, np.log(nDocs/(1+v))) for k, v in nDocsDict[i].items() if v>1)\n",
    "    duration = time.time() - start\n",
    "    print(str(end) + ': ' + str(int(duration//60)) + \" Minute(s), \" + str(int(duration%60)) + \" Second(s).\")        \n",
    "                \n",
    "#print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nDocsDict={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def genGramTFIDF(a,b,g=1,weights=weights['C']): #WIP\n",
    "    a,b=a.split(),b.split()\n",
    "    \n",
    "    a=[' '.join(a[i:i+g]) for i in range(len(a)-g+1)]\n",
    "    b=[' '.join(b[i:i+g]) for i in range(len(b)-g+1)]\n",
    "    \n",
    "    num=np.sum([b.count(x)*weights[g].get(x,0)**2 for x in a if x in b])\n",
    "    au = list(set(a))\n",
    "    bu = list(set(b))\n",
    "    den1=np.linalg.norm(np.array([a.count(x)*weights[g].get(x,0) for x in au]))\n",
    "    den2=np.linalg.norm(np.array([b.count(x)*weights[g].get(x,0) for x in bu]))\n",
    "    den = (den1*den2)\n",
    "    \n",
    "    wordShare=np.sum([b.count(x)*1 for x in a if x not in stops])\n",
    "    len1=np.sum([1 for x in a if x not in stops])\n",
    "    len2=np.sum([1 for x in b if x not in stops])\n",
    "    unW1=np.sum([1 for x in au])\n",
    "    unW2=np.sum([1 for x in bu])\n",
    "    \n",
    "    return num,den,num/den,den1/(den2+0.00000001),wordShare,len1/(len2+0.000000001),\\\n",
    "           len1+len2,unW1/(unW2+0.00000001), wordShare/np.mean([len1,len2])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Category Flags\n",
    "start = time.time()\n",
    "\n",
    "(TT['matchCount'],TT['failCount'],TT['hasCountry'])=zip(*TT.apply(coCos, axis=1))\n",
    "print(\"Country Match Complete\")\n",
    "(TT['NmatchCount'],TT['NfailCount'],TT['hasNum'])=zip(*TT.apply(coNums, axis=1))\n",
    "\n",
    "duration = time.time() - start\n",
    "print(str(int(duration//60)) + \" Minute(s), \" + str(int(duration%60)) + \" Second(s).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Making sure I have good Grammer\n",
    "start = time.time()\n",
    "for end in ends:\n",
    "    for i in range(1,maxGram+1):\n",
    "        (TT['GIDFNum'+end+str(i)],\n",
    "         TT['GIDFDen'+end+str(i)],\n",
    "         TT['GIDF'+end+str(i)],\n",
    "         TT['GIDFDenDiff'+end+str(i)],\n",
    "         TT['WordShare'+end+str(i)],\n",
    "         TT['LenDiff'+end+str(i)],\n",
    "         TT['LenTot'+end+str(i)],\n",
    "         TT['unWRat'+end+str(i)],\n",
    "         TT['WSpct'+end+str(i)],\n",
    "        )= \\\n",
    "        zip(*TT.apply(lambda row: genGramTFIDF(row['q1'+end],row['q2'+end],i,weights[end]),axis=1))\n",
    "    duration = time.time() - start\n",
    "    print(str(end) + ': ' + str(int(duration//60)) + \" Minute(s), \" + str(int(duration%60)) + \" Second(s).\")\n",
    "duration = time.time() - start\n",
    "print(str(int(duration//60)) + \" Minute(s), \" + str(int(duration%60)) + \" Second(s).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#WordVecFeatures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target='is_duplicate'\n",
    "predictors = [x for x in list(TT) if x not in ['id','qid1','qid2','question1','question2','is_duplicate','rdSimPct', \n",
    "                                                  'preds', 'question1C','question2C', 'test_id', 'isTrain','q1Nums', 'trend',\n",
    "                                               'q2Nums'] + ['q1'+end for end in ends] + ['q2'+end for end in ends]\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max(TT['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.isnan(TT.loc[50000,'id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TT['trend'] = TT.apply(lambda row: row['id'] if not np.isnan(row['id']) else 404289*1.1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# store = HDFStore('../output/TT.h5')\n",
    "\n",
    "# store['TT'] = TT  # save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TT.to_pickle('../output/2GTT0422.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TT=pd.read_pickle('../output/2GTT0421.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del TT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictors = [x for x in predictors if ('3' not in x) and ('4' not in x) and ('5' not in x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TT[[x for x in list(TT) if ('3' not in x) and ('4' not in x) and ('5' not in x)]].to_pickle('../output/2GTT.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "selectTrain=TT['isTrain']==1\n",
    "selectTest=TT['isTrain']==0\n",
    "\n",
    "x_train=TT[selectTrain][predictors]\n",
    "y_train=TT[selectTrain]['is_duplicate'].values\n",
    "\n",
    "# pos_train = x_train[y_train == 1]\n",
    "# neg_train = x_train[y_train == 0]\n",
    "\n",
    "if not just_train:\n",
    "    x_test=TT[selectTest][predictors]\n",
    "    x_test_ids=TT[selectTest]['test_id']\n",
    "\n",
    "# # Now we oversample the negative class\n",
    "# # There is likely a much more elegant way to do this...\n",
    "# p = 0.165\n",
    "# scale = ((len(pos_train) / (len(pos_train) + len(neg_train))) / p) - 1\n",
    "# while scale > 1:\n",
    "#     neg_train = pd.concat([neg_train, neg_train])\n",
    "#     scale -=1\n",
    "# neg_train = pd.concat([neg_train, neg_train[:int(scale * len(neg_train))]])\n",
    "# print(len(pos_train) / (len(pos_train) + len(neg_train)))\n",
    "\n",
    "# #Now we undersample the positive class\n",
    "# pos_size = round(0.165*neg_train.shape[0]/0.835)\n",
    "# pos_train=pos_train.sample(pos_size,random_state=4242)\n",
    "\n",
    "# x_train = pd.concat([pos_train, neg_train])\n",
    "# y_train = (np.zeros(len(pos_train)) + 1).tolist() + np.zeros(len(neg_train)).tolist()\n",
    "\n",
    "# print(len(pos_train) / (len(pos_train) + len(neg_train)))\n",
    "# del pos_train, neg_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# TT[predictors]=TT[predictors].fillna(0)\n",
    "\n",
    "# scaler = MinMaxScaler().fit(TT[predictors])\n",
    "\n",
    "# X = pd.DataFrame(scaler.transform(TT[selectTrain][predictors]),columns=predictors)\n",
    "# y = TT[selectTrain]['is_duplicate']\n",
    "\n",
    "# x_train, x_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# x_train.shape, x_valid.shape, y_train.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.33, random_state=4242)\n",
    "\n",
    "#Now oversample within splits\n",
    "####For train set\n",
    "pos_train = x_train[y_train == 1]\n",
    "neg_train = x_train[y_train == 0]\n",
    "\n",
    "neg_train = neg_train.sample(round(0.835*len(pos_train)/0.165),replace=True)\n",
    "\n",
    "x_train = pd.concat([pos_train, neg_train])\n",
    "y_train = (np.zeros(len(pos_train)) + 1).tolist() + np.zeros(len(neg_train)).tolist()\n",
    "\n",
    "print(len(pos_train) / (len(pos_train) + len(neg_train)))\n",
    "\n",
    "####For validation set\n",
    "pos_valid = x_valid[y_valid == 1]\n",
    "neg_valid = x_valid[y_valid == 0]\n",
    "\n",
    "neg_valid = neg_valid.sample(round(0.835*len(pos_valid)/0.165),replace=True)\n",
    "\n",
    "x_valid = pd.concat([pos_valid, neg_valid])\n",
    "y_valid = (np.zeros(len(pos_valid)) + 1).tolist() + np.zeros(len(neg_valid)).tolist()\n",
    "\n",
    "print(len(pos_valid) / (len(pos_valid) + len(neg_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "bst = XGBClassifier(\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=500,\n",
    "    max_depth=7,\n",
    "    min_child_weight=5,\n",
    "    gamma=1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective='binary:logistic',\n",
    "    seed=27\n",
    ")\n",
    "\n",
    "d_train = xgb.DMatrix(x_train, label=y_train)\n",
    "d_valid = xgb.DMatrix(x_valid, label=y_valid)\n",
    "if not just_train:\n",
    "    d_test = xgb.DMatrix(x_test)\n",
    "\n",
    "watchlist = [(x_train, y_train), (x_valid,y_valid)]\n",
    "\n",
    "print(\"Starting XGB\")\n",
    "\n",
    "bst.fit(x_train[predictors], y_train, eval_metric='logloss', verbose=10, eval_set=watchlist, early_stopping_rounds=50)\n",
    "\n",
    "duration = time.time() - start\n",
    "print(str(int(duration//60)) + \" Minute(s), \" + str(int(duration%60)) + \" Second(s).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(metrics.log_loss(y_valid, bst.predict_proba(x_valid)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (12.0, 40.0)\n",
    "xgb.plot_importance(bst); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "submit=pd.DataFrame()\n",
    "submit['test_id']=np.int32(x_test_ids)\n",
    "submit['is_duplicate']=bst.predict_proba(x_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submit.to_csv('../output/submission_0423_Rel3_2G_3.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = {}\n",
    "params['objective'] = 'binary:logistic'\n",
    "params['eval_metric'] = 'logloss'\n",
    "params['eta'] = 0.05\n",
    "params['max_depth'] = 7\n",
    "params['subsample']=0.5\n",
    "\n",
    "d_train = xgb.DMatrix(x_train, label=y_train)\n",
    "d_valid = xgb.DMatrix(x_valid, label=y_valid)\n",
    "if not just_train:\n",
    "    d_test = xgb.DMatrix(x_test)\n",
    "\n",
    "watchlist = [(x_train, y_train), (x_valid,y_valid)]\n",
    "bst = xgb.train(params, d_train, 1000, watchlist, early_stopping_rounds=50, verbose_eval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####HyperParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set our parameters for xgboost\n",
    "start = time.time()\n",
    "\n",
    "xgb1 = XGBClassifier(\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=1000,\n",
    "    max_depth=5,\n",
    "    min_child_weight=1,\n",
    "    gamma=1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective='binary:logistic',\n",
    "    seed=27\n",
    ")\n",
    "    \n",
    "    \n",
    "# params = {}\n",
    "# params['objective'] = 'binary:logistic'\n",
    "# params['eval_metric'] = 'logloss'\n",
    "# params['eta'] = 0.05\n",
    "# params['max_depth'] = 4\n",
    "# params['subsample']=0.5\n",
    "\n",
    "d_train = xgb.DMatrix(x_train, label=y_train)\n",
    "d_valid = xgb.DMatrix(x_valid, label=y_valid)\n",
    "if not just_train:\n",
    "    d_test = xgb.DMatrix(x_test)\n",
    "\n",
    "    \n",
    "# modelfit(xgb1, d_train, predictors)\n",
    "# watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "\n",
    "# bst = xgb.train(params, d_train, 1000, watchlist, early_stopping_rounds=50, verbose_eval=10)\n",
    "\n",
    "duration = time.time() - start\n",
    "print(str(int(duration//60)) + \" Minute(s), \" + str(int(duration%60)) + \" Second(s).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "useTrainCV=True\n",
    "cv_folds=5\n",
    "early_stopping_rounds=50\n",
    "alg=xgb1\n",
    "\n",
    "if useTrainCV:\n",
    "    xgb_param = alg.get_xgb_params()\n",
    "    xgtrain = d_train #xgb.DMatrix(dtrain[predictors].values,label=dtrain[target].values)\n",
    "    cvresult=xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds, \n",
    "                    metrics='logloss', early_stopping_rounds=early_stopping_rounds, verbose_eval=10)\n",
    "    alg.set_params(n_estimators=cvresult.shape[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Fit the algorithm\n",
    "alg.fit(x_train[predictors], y_train, eval_metric='logloss', verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Predict Train\n",
    "print(metrics.log_loss(y_valid, alg.predict_proba(x_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "param_test1={\n",
    "    'max_depth': list(range(3,10,2)),\n",
    "    'min_child_weight': list(range(1,6,2))\n",
    "}\n",
    "\n",
    "gsearch1 = GridSearchCV(estimator=XGBClassifier(learning_rate=0.1, n_estimators=1000, max_depth=5, min_child_weight=1, gamma=0,\n",
    "                                               subsample=0.8, colsample_bytree=0.8, objective='binary:logistic',\n",
    "                                               scale_pos_weight=1, seed=27),\n",
    "                       param_grid=param_test1, scoring='neg_log_loss', iid=False, cv=5, verbose=10, n_jobs=4)\n",
    "gsearch1.fit(x_train[predictors],y_train)\n",
    "\n",
    "duration = time.time() - start\n",
    "print(str(int(duration//60)) + \" Minute(s), \" + str(int(duration%60)) + \" Second(s).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics.log_loss(y_valid, bst.predict(d_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (12.0, 40.0)\n",
    "xgb.plot_importance(alg); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submit=pd.DataFrame()\n",
    "submit['test_id']=x_test_ids\n",
    "submit['is_duplicate']=bst.predict(d_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submit['test_id']=submit.apply(lambda row: int(row['test_id']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "submit.to_csv('../output/submission_0422_Rel3_2G_2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = 0.165 / 0.37\n",
    "b = (1 - 0.165) / (1 - 0.37)\n",
    "def convToLB(x):\n",
    "    return a*x/(a*x+b*(1-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submit['is_duplicate']=submit.apply(lambda row: convToLB(row['is_duplicate']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(submit[submit['is_duplicate']>0.5])/len(submit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#@500 all\n",
    "#0.360219\n",
    "#.403\n",
    "#5k, 0.36665\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#0.477\n",
    "#Adding stemming: 0.487\n",
    "#Add Flag: 0.484\n",
    "#+HasCountry: 0.483\n",
    "#Fixing country flag: 0.479\n",
    "#+NumFlag: 0.471\n",
    "#Stemmed and unstemmed: 0.448\n",
    "#Fixing 'end' flags on features: 0.445\n",
    "#0.421\n",
    "#0.4018\n",
    "#Adding ReplDict: 0.398159 -- worse than above on test\n",
    "# lower eta: 0.395459\n",
    "#+2G: 0.378047\n",
    "#+Trend: 0.378762\n",
    "\n",
    "#Try a train/test split on old/new\n",
    "\n",
    "#+5G and Stemmed/unstemmed train: 0.364\n",
    "#+Lemmatizing: 0.357\n",
    "#depth ->6: 0.356\n",
    "#Adding POS: 0.345\n",
    "#Lower eta, raise maxnum: 0.339\n",
    "#lower eta, 0.334\n",
    "\n",
    "#1TFIDF .517835\n",
    "#Adding num and den: 0.513763\n",
    "#log->v+10k: 0.500909\n",
    "#Back to Log, normalized nums: 0.501241 +RF ->0.499851\n",
    "#making / spaces etc: XXXXXXXXXX\n",
    "#putting stopwords back in: 0.521442\n",
    "#Real TFIDF: 0.513535\n",
    "#Real TFIDF w/testQs: 0.513451\n",
    "\n",
    "\n",
    "\n",
    "#TFIDF on 5: 0.418284\n",
    "#Real TFIDF on 5: ~0.407"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#NV#\n",
    "notafunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_train = xgb.DMatrix(X, label=y_train)\n",
    "train['preds']=bst.predict(d_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.distplot(train.preds[0:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train['xgbPreds']=train['preds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators=15, max_depth=6, random_state=0)\n",
    "clf = BaggingRegressor(rf, n_estimators=45, max_samples=0.1, random_state=25)\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_valid)\n",
    "\n",
    "NV=pd.DataFrame()\n",
    "NV['preds']=y_pred\n",
    "NV['valid']=y_valid.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"LogLoss Score (Train): %f\" % metrics.log_loss(NV['valid'],NV['preds'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.distplot(NV.preds[0:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TT.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TT.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
